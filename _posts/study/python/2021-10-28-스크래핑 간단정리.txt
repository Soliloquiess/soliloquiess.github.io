
- 웹 스크래핑: 필요한 정보만 파싱해서 가져오는거
- 웹 크롤링: 사이트 모든 내용 가져오는거

```
/학교/학년/반/학생[2]
//*[@학번="1-1-5"]

//*[@id="account"]/a
/html/body/div[2]/div[2]/div[3]/div/div[2]/a

/html/body/div/div/div[3]/div/div/span/a...
//*[@id="login"]


<학교 이름="나도고등학교">
    <학년 value="1학년">
        <반 value="1반">
            <학생 value="1번" 학번="1-1-1">이지은</학생>
            <학생 value="2번" 학번="1-1-2">유재석</학생>
            <학생 value="3번" 학번="1-1-3">조세호</학생>
            <학생 value="4번" 학번="1-1-4">박명수</학생>
            <학생 value="5번" 학번="1-1-5">이지은</학생>
        </반>
        <반 value="2반"/>
        <반 value="3반"/>
        <반 value="4반"/>
    </학년>

    <학년 value="2학년"/> ... 3반 유재석 <...>
    <학년 value="3학년"/>
</학교>
```

이런 코드가 있으면 xpath를 써서

/학교/학년/반/학생[2] 이런식으로 가져올 수 있게 되는 것.

혹은 학번처럼 //*[@학번="1-1-5"] 이런식으로 가져오기가 가능하다는 뜻
만약 페이지에서
/html/body/div/div/div[3]/div/div/span/a...

이런식으로 엄청 길게 있다? 이걸
//*[@id="login"] 이런식으로 줄이기가 가능하다는 뜻


//*[@id="account"]/a
/html/body/div[2]/div[2]/div[3]/div/div[2]/a

/html/body/div/div/div[3]/div/div/span/a...
//*[@id="login"]

/는 내가 위치한 곳에서 한단계 아래, 2번 사용하면 아래 전부를 찾아봄
*(애스터리스크)는 학교태그,학년태그 반 태그 상관없이 다 찾는다.

//학생[@학번="1-1-5"] 
만약 이렇게 쓰면 학번태그중 찾는데,

//*[@학번="1-1-5"]
이렇게 쓰게 되면 학번을 모든 태그에서 찾는다.

![20221028_000443](https://user-images.githubusercontent.com/37941513/198606015-23341111-4583-45c2-82ca-0b9124716a87.png)


Xpath는 주소 내 특정 element를 지칭하는 주소라고 생각하면 된다.



#### UserAgent

자세한 설명: 
- https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent


https://www.whatismybrowser.com/detect/what-is-my-user-agent/

여기 가면 mozilla로 시작하는 string이 나오는데 이게 해당 브라우저 UserAgent정보다.
만약 크롬이 아니라 익스플로러면?

좀 다르게 뜬다.

![20221029_194751](https://user-images.githubusercontent.com/37941513/198827203-87431555-0c6d-41ce-8c6f-c542a321ef94.png)

유저에이전트 주기 전에는 티스토리나 구글 사이트 등 브라우저에서 막았는데 유저 에이전트 줘야 실제로 크롬에서 유의미한 결과 받아올 수 있게 됨.


크롬에서와 익스플로러 이런식으로 접속하는 게 좀 다르다.


그리고 이제 진짜 스크래핑 하기 위해

pip install BeautifulSoup
pip install lxml

를 터미널에 입력해준다.

만약 안 될시 pip 업그레이드 해줌

python.exe -m pip install --upgrade pip

근데 만약 파이썬 3 면 BeautifulSoup3은 작동이 안된다. BeautifulSoup4 설치



pip install beautifulsoup4

pip install bs4

위 문구가 안되면 아래 문구 설치

<br>

-----

HTTP Method

Get = url에 (주소에) 정보를 실어서 보냄(누구나 볼 수 있음)
Post = url이 아닌 http body에 정보를 숨겨서 보냄.




![20221030_143803](https://user-images.githubusercontent.com/37941513/198864231-3860bcef-9f84-4552-8863-58a30d9d4df2.png)


테이블 밑에 (table태그) tr정보 가져오고 그 밑에 td정보 가져온다.

테이블 태그 가져오기위해 클래스 가져옴.()


ata_rows = soup.find("table", attrs={"class":"type_2"}).find("tbody").find_all("tr")
#find를 통해 첫번쨰 tbody 가져오고 여기 밑에있는 tr가져옴. 이 정보를 변수명에 넣고 이걸  rows라 변수명 붙임.


![20221030_144335](https://user-images.githubusercontent.com/37941513/198864381-0545e730-bc33-4275-b92c-9d772f5b85d3.png)


근데 가져오니까 이런 빈줄 있는게 몇 있다 이거 없애려면?

이런 긴 줄 같은건 tr이 있고 밑에 tr이 있는데 줄바꿈 하기 위해서 넣어둔 것.
우리에겐 의미 없는 것. 지금 td가 하나 있는데 
tr이 하나 이상이면 스킵하게 하자

```
    data_rows = soup.find("table", attrs={"class":"type_2"}).find("tbody").find_all("tr")
    for row in data_rows:
        columns = row.find_all("td")
        if len(columns) <= 1: # 의미 없는 데이터는 skip(의미없는 tr밑에 td있는거)
            continue
        data = [column.get_text().strip() for column in columns]    #get_text().strip()를 붙여 불필요한 줄바꿈 제거
        #print(data)
        writer.writerow(data)

```


<br>

-----

그리고 셀레니움 쓰기 전에 https://chromedriver.chromium.org/downloads
에서 크롬 드라이버 다운받자

이게 크롬 제어가 가능한 드라이버

그리고 셀레니움 설치

pip install selenium

그리고 현 셀레니움은 크롬 드라이버 103버전 이상은 뭔가 버전 에러나느거 같다 이거 이하로 받자.

근데 알고보니 버전을 크롬 버전과 맞춰야 하던거.. 아..

chrome://version/

가서 버전 호환되는 크롬 드라이버 다운 후 실행
from selenium.webdriver.common.keys import Keys
from selenium import webdriver
browser = webdriver.Chrome()

elem = browser.find_element(By.CLASS_NAME, 'link_login') 
elem.click()
browser.back()
browser.forward()
browser.refresh()
elem = browser.find_element_by_id("query")
from selenium.webdriver.common.keys import keys
elem.send_keys("abcd")
elem.send_keys(Keys.ENTER)
elem = browser.find_element(By.ID, 'query') 
elem = browser.find_element(By.TAG_NAME,"a") 

이전과 셀레니움 버전이 달라서 위 처럼 사용해야 한다.

for e in elem: 
    elem.get_attribute("href")

elem = browser.find_element_by_xpath("//*[@id='daumSearch']/fieldset/div/div/button[2]")